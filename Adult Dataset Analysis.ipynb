{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier, KernelDensity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data into Dataframe\n",
    "df1 = pd.read_csv('adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Rows with Missing Values\n",
    "#Renaming Columns for Legibility\n",
    "df1.replace({' ?': np.NaN}, inplace=True)\n",
    "df1 = df1.dropna()\n",
    "adultdf1 = df1\n",
    "adultdf1 = adultdf1.rename(columns={' State-gov': 'State_gov'})\n",
    "adultdf1 = adultdf1.rename(columns={' Bachelors': 'Bachelors'})\n",
    "adultdf1 = adultdf1.rename(columns={' Never-married': 'Marital_status'})\n",
    "adultdf1 = adultdf1.rename(columns={' Adm-clerical': 'Adm_clerical'})\n",
    "adultdf1 = adultdf1.rename(columns={' Not-in-family': 'Family_status'})\n",
    "adultdf1 = adultdf1.rename(columns={' 2174': 'Capital_gain'})\n",
    "adultdf1 = adultdf1.rename(columns={' 0': 'Capital_loss'})\n",
    "adultdf1 = adultdf1.rename(columns={' 40': 'Hours/week'})\n",
    "adultdf1 = adultdf1.rename(columns={' United-States': 'Country'})\n",
    "adultdf1 = adultdf1.rename(columns={' <=50K': 'Income'})\n",
    "\n",
    "#First, label encoding the state-gov column and then converting into one-hot-encoded columns\n",
    "labelencoder = LabelEncoder()\n",
    "adultdf1['State_gov'] = labelencoder.fit_transform(adultdf1['State_gov'])\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(adultdf1[['State_gov']]).toarray())\n",
    "\n",
    "#Converted education year column into numerical column using .map. \n",
    "#Since was ordinal data, did not one-hot-encode.\n",
    "year_ord_map = {' Preschool': 1, ' 1st-4th': 2, ' 5th-6th': 3, \n",
    "               ' 7th-8th': 4, ' 9th': 5, ' 10th': 6, ' 11th': 7, ' 12th': 8, ' HS-grad': 9, ' Some-college': 10, ' Assoc-voc': 11,\n",
    "                ' Assoc-acdm' : 12, ' Bachelors': 13, ' Prof-school': 14, ' Masters': 15, ' Doctorate': 16}\n",
    "adultdf1['Bachelors'] = adultdf1['Bachelors'].map(year_ord_map)\n",
    "\n",
    "#One-hot-encoded marital status; first put encoded data into seperate dataframe.\n",
    "adultdf1['Marital_status'] = labelencoder.fit_transform(adultdf1['Marital_status'])\n",
    "enc2 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc2_df = pd.DataFrame(enc2.fit_transform(adultdf1[['Marital_status']]).toarray())\n",
    "\n",
    "#One-hot-encoded job; first put encoded data into seperate dataframe.\n",
    "adultdf1['Adm_clerical'] = labelencoder.fit_transform(adultdf1['Adm_clerical'])\n",
    "enc3 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc3_df = pd.DataFrame(enc3.fit_transform(adultdf1[['Adm_clerical']]).toarray())\n",
    "\n",
    "#One-hot-encoded family status; first put encoded data into seperate dataframe.\n",
    "adultdf1['Family_status'] = labelencoder.fit_transform(adultdf1['Family_status'])\n",
    "enc4 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc4_df = pd.DataFrame(enc4.fit_transform(adultdf1[['Family_status']]).toarray())\n",
    "\n",
    "#One-hot-encoded race; first put encoded data into seperate dataframe.\n",
    "adultdf1[' White'] = labelencoder.fit_transform(adultdf1[' White'])\n",
    "enc5 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc5_df = pd.DataFrame(enc5.fit_transform(adultdf1[[' White']]).toarray())\n",
    "\n",
    "#One-hot-encoded Gender; first put encoded data into seperate dataframe.\n",
    "adultdf1[' Male'] = labelencoder.fit_transform(adultdf1[' Male'])\n",
    "enc6 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc6_df = pd.DataFrame(enc6.fit_transform(adultdf1[[' Male']]).toarray())\n",
    "\n",
    "#one-hot-encoded Country; first put encoded data into seperate dataframe.\n",
    "adultdf1['Country'] = labelencoder.fit_transform(adultdf1['Country'])\n",
    "enc7 = OneHotEncoder(handle_unknown='ignore')\n",
    "enc7_df = pd.DataFrame(enc7.fit_transform(adultdf1[['Country']]).toarray())\n",
    "\n",
    "#Dropped all the numerical columns\n",
    "adultdf1 = adultdf1.drop(['State_gov'], axis = 1)\n",
    "adultdf1 = adultdf1.drop(['Marital_status'], axis = 1)\n",
    "adultdf1 = adultdf1.drop(['Adm_clerical'], axis = 1)\n",
    "adultdf1 = adultdf1.drop(['Family_status'], axis = 1)\n",
    "adultdf1 = adultdf1.drop([' White'], axis = 1)\n",
    "adultdf1 = adultdf1.drop([' Male'], axis = 1)\n",
    "adultdf1 = adultdf1.drop(['Country'], axis = 1)\n",
    "\n",
    "#Dropped the columns which had no significance to the data. If we kept this data, it would skew some data points.\n",
    "adultdf1 = adultdf1.drop(columns=[' 13'])\n",
    "adultdf1 = adultdf1.drop(columns=['Capital_gain'])\n",
    "adultdf1 = adultdf1.drop(columns=['Capital_loss'])\n",
    "\n",
    "#Combined all the one-hot-encoded data into one list so easier to join later on.\n",
    "dfs = [enc_df, enc2_df, enc3_df, enc4_df, enc5_df, enc6_df, enc7_df]\n",
    "\n",
    "#Changed income to class label 1 and 0 using .map method.\n",
    "income_ord_map = {' <=50K': 0, ' >50K': 1}\n",
    "adultdf1['Income'] = adultdf1['Income'].map(income_ord_map)\n",
    "\n",
    "#Combined original dataframe with one-hot-encoded data. Now all data is numerical and stored in adultdf1.\n",
    "dfss = pd.concat(dfs, axis = 1)\n",
    "adultdf1 = adultdf1.reset_index(drop=True)\n",
    "adultdf1 = dfss.join(adultdf1)\n",
    "adultdf1 = adultdf1.dropna()\n",
    "\n",
    "#Converted dataframe into array.\n",
    "adult_arr = adultdf1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy function. Compares predicted labels vs actual labels and counts accuracy.\n",
    "def prediction(predictions, Y_given):\n",
    "    wrong = 0\n",
    "    counter = 0\n",
    "    for test, train in zip(predictions, Y_given):\n",
    "        if test == train:\n",
    "            wrong = wrong\n",
    "        else:\n",
    "            wrong = wrong + 1\n",
    "        counter = counter + 1\n",
    "    accuracy = 1 - (wrong/counter)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.1, 'kernel': 'linear'}\n",
      "Train accuracy: 0.8344\n",
      "Test accuracy: 0.8275903183498271\n",
      "Best params: {'C': 0.1, 'kernel': 'linear'}\n",
      "Train accuracy: 0.8314\n",
      "Test accuracy: 0.8254838837884027\n",
      "Best params: {'C': 0.1, 'kernel': 'linear'}\n",
      "Train accuracy: 0.838\n",
      "Test accuracy: 0.826437740948293\n"
     ]
    }
   ],
   "source": [
    "#Three lists to store training accuracy for each trial, test accuracy for each trial, and best parameters for each trial.\n",
    "SVM_train_accuracy = []\n",
    "SVM_test_accuracy = []\n",
    "SVM_best_params = []\n",
    "\n",
    "#Each trial\n",
    "for i in range(3):\n",
    "    data_svm = shuffle(adult_arr)\n",
    "#Data Splitting, train-test-split    \n",
    "    X_svm = data_svm[:, 0:-1]\n",
    "    Y_svm = data_svm[:, -1]\n",
    "    X_train_svm, X_test_svm, Y_train_svm, Y_test_svm = train_test_split(X_svm, Y_svm, test_size = 25161/30161, random_state=42,\n",
    "                                                                        stratify = Y_svm)\n",
    "#Scaling training data using StandardScaler\n",
    "    scaler_svm = preprocessing.StandardScaler().fit(X_train_svm)\n",
    "    X_train_svm = scaler_svm.transform(X_train_svm)\n",
    "#Param-grid\n",
    "    parameters = [{'kernel': ['rbf'], 'gamma': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0], 'C': [10**-7, 10**-6, 10**-5,\n",
    "                                                                                                     10**-4, 10**-3, 10**-2,\n",
    "                                                                                                     10**-1]},\n",
    "                  {'kernel': ['poly'], 'degree': [2, 3], 'C': [10**-7, 10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1]},\n",
    "                  {'kernel': ['linear'], 'C': [10**-7, 10**-6, 10**-5, 10**-4, 10**-3, 10**-2, 10**-1]}]\n",
    "#SVC class, grid search, and fit training data and training labels\n",
    "    svc = svm.SVC(gamma = 'auto')\n",
    "    grid_search_svm = GridSearchCV(svc, parameters, cv = 5, error_score = np.nan)\n",
    "    grid_search2_svm = grid_search_svm.fit(X_train_svm, Y_train_svm)\n",
    "#Printing and Storing best params into list.\n",
    "    best_params_svm = grid_search2_svm.best_params_\n",
    "    print(\"Best params: {}\".format(best_params_svm))\n",
    "    SVM_best_params.append('Trial ' + str(i + 1) + ': ' + str(best_params_svm))\n",
    "#Best estimator is refitted so just used .predict on training data to find predicted training values.\n",
    "#Used prediction pre-defined function to count accuracy on predicted training set.\n",
    "    train_predictions_svm = grid_search2_svm.best_estimator_.predict(X_train_svm) #.best_estimator_\n",
    "    train_accuracy_svm = prediction(train_predictions_svm, Y_train_svm)\n",
    "#Printing and storing training accuracy into list.\n",
    "    print(\"Train accuracy: {}\".format(train_accuracy_svm))\n",
    "    SVM_train_accuracy.append('Trial ' + str(i + 1) + ': ' + str(train_accuracy_svm))\n",
    "#Scaling testing data with my training data scaler. This ensures training and testing data are scaled the same.\n",
    "#Printing and storing test accuracy into list.\n",
    "    X_test_svm = scaler_svm.transform(X_test_svm)\n",
    "    test_predictions_svm = grid_search2_svm.best_estimator_.predict(X_test_svm) #.best_estimator_\n",
    "    test_accuracy_svm = prediction(test_predictions_svm, Y_test_svm)\n",
    "    print(\"Test accuracy: {}\".format(test_accuracy_svm))\n",
    "    SVM_test_accuracy.append('Trial ' + str(i + 1) + ': ' + str(test_accuracy_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'metric': 'minkowski', 'n_neighbors': 416, 'weights': 'distance'}\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8241325861452248\n",
      "Best params: {'metric': 'minkowski', 'n_neighbors': 458, 'weights': 'distance'}\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8241723301935535\n",
      "Best params: {'metric': 'minkowski', 'n_neighbors': 312, 'weights': 'distance'}\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8200786932156909\n"
     ]
    }
   ],
   "source": [
    "#Three lists to store training accuracy for each trial, test accuracy for each trial, and best parameters for each trial.\n",
    "kNN_train_accuracy = []\n",
    "kNN_test_accuracy = []\n",
    "kNN_best_params = []\n",
    "\n",
    "#Setting K parameters\n",
    "p = (np.linspace(1,500,25))\n",
    "p = p.astype('int64')\n",
    "\n",
    "#For Each Trial\n",
    "for i in range(3):\n",
    "    data_knn = shuffle(adult_arr)\n",
    "#Data Splitting, Train-test-split\n",
    "    X_knn = data_knn[:, 0:-1]\n",
    "    Y_knn = data_knn[:, -1]\n",
    "    X_train_knn, X_test_knn, Y_train_knn, Y_test_knn = train_test_split(X_knn, Y_knn, test_size = 25161/30161, random_state=42,\n",
    "                                                                        stratify = Y_knn)\n",
    "#Scaling training data using StandardScaler\n",
    "    scaler_knn = preprocessing.StandardScaler().fit(X_train_knn)\n",
    "    X_train_knn = scaler_knn.transform(X_train_knn)\n",
    "#Param-grid, initializing KNN Class, and fitting with gridsearch\n",
    "#Printing and storing best params in list\n",
    "    params = [{'weights' : ['uniform', 'distance'], 'metric' : ['minkowski'],'n_neighbors': p}]\n",
    "    neighbor = KNeighborsClassifier()\n",
    "    grid_search_knn = GridSearchCV(neighbor, params, cv=5, error_score = np.nan)\n",
    "    grid_search_knn2 = grid_search_knn.fit(X_train_knn, Y_train_knn)\n",
    "    best_params_knn = grid_search_knn2.best_params_\n",
    "    print(\"Best params: {}\".format(best_params_knn))\n",
    "    kNN_best_params.append('Trial ' + str(i + 1) + ': ' + str(best_params_knn))\n",
    "#Predicting training data with best_estimator\n",
    "#Calculating training accuracy with defined prediction function\n",
    "#Printing and storing training accuracy in list.\n",
    "    train_predictions_knn = grid_search_knn2.best_estimator_.predict(X_train_knn)\n",
    "    train_accuracy_knn = prediction(train_predictions_knn, Y_train_knn)\n",
    "    print(\"Train accuracy: {}\".format(train_accuracy_knn))\n",
    "    kNN_train_accuracy.append('Trial ' + str(i + 1) + ': ' + str(train_accuracy_knn))\n",
    "#Scaling testing data with my training data scaler. This ensures training and testing data are scaled the same.\n",
    "#Printing and storing test accuracy into list.\n",
    "    X_test_knn = scaler_knn.transform(X_test_knn)\n",
    "    test_predictions_knn = grid_search_knn2.best_estimator_.predict(X_test_knn)\n",
    "    test_accuracy_knn = prediction(test_predictions_knn, Y_test_knn)\n",
    "    print(\"Test accuracy: {}\".format(test_accuracy_knn))\n",
    "    kNN_test_accuracy.append('Trial ' + str(i + 1) + ': ' + str(test_accuracy_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest-Transformed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_features': 16, 'n_estimators': 1024}\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8247684909184849\n",
      "Best params: {'max_features': 8, 'n_estimators': 1024}\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8231787289853345\n",
      "Best params: {'max_features': 20, 'n_estimators': 1024}\n",
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.8244902825801836\n"
     ]
    }
   ],
   "source": [
    "#Three lists to store training accuracy for each trial, test accuracy for each trial, and best parameters for each trial.\n",
    "rf_train_accuracy = []\n",
    "rf_test_accuracy = []\n",
    "rf_best_params = []\n",
    "\n",
    "#For each trial\n",
    "for i in range(3):\n",
    "    data_rf = shuffle(adult_arr)\n",
    "#Data splitting, train-test-split\n",
    "    X_rf = data_rf[:, 0:-1]\n",
    "    Y_rf = data_rf[:, -1]\n",
    "    X_train_rf, X_test_rf, Y_train_rf, Y_test_rf = train_test_split(X_rf, Y_rf, test_size = 25161/30161, random_state=42)#, stratify = Y_rf)\n",
    "#Param-grid, initialize random forest class, and fitting training data with grid search.\n",
    "    params = [{'n_estimators' : [1024], 'max_features' : [1, 2, 4, 6, 8, 12, 16, 20]}]\n",
    "    forest = RandomForestClassifier()\n",
    "    grid_search_rf = GridSearchCV(forest, params, cv=5, error_score = np.nan)\n",
    "    grid_search_rf2 = grid_search_rf.fit(X_train_rf, Y_train_rf)\n",
    "#Printing and storing best params into list.\n",
    "    best_params_rf = grid_search_rf2.best_params_\n",
    "    print(\"Best params: {}\".format(best_params_rf))\n",
    "    rf_best_params.append('Trial ' + str(i + 1) + ': ' + str(best_params_rf))\n",
    "#Using best_estimator to predict training data.\n",
    "#Storing and printing training accuracy into list.\n",
    "#Using defined prediction function to calculate total accuracy.\n",
    "    train_predictions_rf = grid_search_rf2.best_estimator_.predict(X_train_rf)\n",
    "    train_accuracy_rf = prediction(train_predictions_rf, Y_train_rf)\n",
    "    print(\"Train accuracy: {}\".format(train_accuracy_rf))\n",
    "    rf_train_accuracy.append('Trial ' + str(i + 1) + ': ' + str(train_accuracy_rf))\n",
    "#Predicting Test data with best_estimator.predict.\n",
    "#Using prediction function to calculate accuracy.\n",
    "#Printing and storing Test accuracy into list.\n",
    "    test_predictions_rf = grid_search_rf2.best_estimator_.predict(X_test_rf)\n",
    "    test_accuracy_rf = prediction(test_predictions_rf, Y_test_rf)\n",
    "    print(\"Test accuracy: {}\".format(test_accuracy_rf))\n",
    "    rf_test_accuracy.append('Trial ' + str(i + 1) + ': ' + str(test_accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest-Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf2_train_accuracy = []\n",
    "# rf2_test_accuracy = []\n",
    "# rf2_best_params = []\n",
    "# df1 = df1.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# df1_arr = df1.to_numpy()\n",
    "\n",
    "# for i in range(1):\n",
    "#     data_rf2 = shuffle(df1_arr)\n",
    "#     X_rf2 = data_rf2[:, 0:-1]\n",
    "#     Y_rf2 = data_rf2[:, -1]\n",
    "#     X_train_rf2, X_test_rf2, Y_train_rf2, Y_test_rf2 = train_test_split(X_rf2, Y_rf2, test_size = 25161/30161, random_state=42)\n",
    "#     params_rf2 = [{'n_estimators' : [1024], 'max_features' : [1, 2, 4, 6, 8, 12, 16, 20]}]\n",
    "#     forest_rf2 = RandomForestClassifier()\n",
    "#     grid_search_rf2 = GridSearchCV(forest_rf2, params_rf2, cv=5, error_score = np.nan)\n",
    "#     grid_search_rf3 = grid_search_rf2.fit(X_train_rf2, Y_train_rf2)\n",
    "#     best_params_rf2 = grid_search_rf3.best_params_\n",
    "#     print(\"Best params: {}\".format(best_params))\n",
    "#     rf2_best_params.append('Trial ' + str(i + 1) + ': ' + str(best_params_rf2))\n",
    "#     train_predictions_rf2 = grid_search_rf3.best_estimator_.predict(X_train_rf2)\n",
    "#     train_accuracy_rf2 = prediction(train_predictions_rf2, Y_train_rf2)\n",
    "#     print(\"Train accuracy: {}\".format(train_accuracy_rf2))\n",
    "#     rf2_train_accuracy.append('Trial ' + str(i + 1) + ': ' + str(train_accuracy_rf2))\n",
    "#     test_predictions_rf2 = grid_search_rf3.best_estimator_.predict(X_test_rf2)\n",
    "#     test_accuracy_rf2 = prediction(test_predictions_rf2, Y_test_rf2)\n",
    "#     print(\"Test accuracy: {}\".format(test_accuracy_rf2))\n",
    "#     rf2_test_accuracy.append('Trial ' + str(i + 1) + ': ' + str(test_accuracy_rf2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
